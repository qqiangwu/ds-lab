# Paxos
这里，实现了两个版本的Paxos，一个是基于状态机的，一个是基于原始协议中Role的。后者见`paxos-sync`分支。

# 为什么要引入Paxos
pbservice提供了可容错服务的一个示例，然而，它仍然不是完美的。

+ viewservice是系统单点，显然，这是不能接受的。我们可以使用pbservice中的primary/backup方式为其实现高可用，然而，这样，我们陷入了一个循环。因此，我们需要一个更加基本的容错原语。
+ pbservice中服务器短暂不可达时，就被标识成died，造成view切换；而view切换的成本非常高，需要全量同步。这本质是一个同步策略问题，我们可以用同步日志的方式/差分的方式，来避免全量同步。

我们考虑如何解决VS的单点问题。由于复制的实质是全序可靠多播，因此，我们使用全序可靠多播即可以解决问题。而全序可靠多播本质上是一个Consensus问题。因此，我们需要先构造一个consensus协议。

# 错误模型
分布式系统的本质之一是容错，因此，我们需要先假设恰当的错误模型。

+ 节点：
    + 永远不会crash
    + 静态成员：节点数不会变化。即不考虑配置变更的问题
+ 网络:
    + Partition
    + Unreliable：消息会丢，会乱序

静态节点的假设极大地简化了错误模型。其中，节点永远不会crash对我们的影响不大，因为，分布式环境中，无法区别crash与network partition，因此，我们使用相同的处理方式来处理它们。成员不会变化，则使得我们免去了一项工作。

在确定了有哪些错误的基础上，考虑如何检测这些错误。

首先，需要明确的是，即使我们使用了可靠的传输协议，也依然会存在消息丢与乱序的问题，只不过，问题出现在应用层。考虑以下两种场景

+ 丢包: client发送请求给A，然后network partition了，无法知道请求是否已经到达A，如果没有到达，即丢包；如果到达了，会出现重复或者乱序的问题
+ 乱序：client发送请求给A，A正忙，之后network partition了，client重试，发请求到了B，B先处理了，client又发送一下条请求; 之后，A又处理了之前pending的请求，于是出现乱序。

# 基于状态机的实现
在开始实现的时候，我没有按照协议规定的那样，使用基于角色的方式来实现，而是用了状态机的方式，即，每一个节点，同一时刻只能处于一个状态，而不是即是acceptor，又是proposer。我认为这用状态机可以把这个复杂的过程更好的展示出来。然而，写完之后，我才发现，整个实现变得特别复杂。比如，处于proposer状态的节点，实质当充当了acceptor与proposer两个角色，于是状态转移变得特别复杂，因此从协议来讲，proposer维护着一个proposal号，而acceptor也维护着一个proposal号。

这也给了我一个启示：当能够顺序地表示一个对象的生命周期时，不要用状态机，除非对象的生命周期事情很复杂。

这里，我大致描述一下实现：

+ 调用paxos.Start(seq, val)时，如果paxos peer不存在，创建之，然后进行proposer状态，开始发起prepare
+ 当收到prepare时，如果paxos peer不存在，创建之，进行voter状态，并投票
+ 接收到majority prepare后，proposer转换成proposed，然后发起accept；否则，转变成voter状态
+ 当收到accept时，如果paxos peer不存在，创建之，投票
+ 接收到majoirty accept后，proposed转换成done状态，并发送decide到所有节点
+ 任意节点收到decide，不论处于什么状态，直接转移到done状态
+ voter在一定时间内没有收到prepare/accept/decide的话，timeout，转移到proposer状态
+ proposer/proposed收到prepare/accept的话，如果对方的proposal号大于自己的，投票，并转移到voter转换

按照下面的描述，画个图就很清楚了。

# 基于角色的实现
按照论文中的角色方式实现一下，发起复杂度比基于状态机的简单多了。唯一比它差的，大概就是每个proposer要单独占据一个线程。而在基于状态机的模型中，除了timeout，并不需要额外的支持。

同时，基于状态机的实现中，整个proposer的过程被分在好几个回调中，逻辑显得十分零乱。而使用基于角色的方式，proposer的工作流种是线性的，十分直观，出错的可能性也小了许多。

# 通用问题
这里，描述几个Paxos实现时需要考虑的问题

+ Proposal号码的唯一性：实际上，proposal的值并不需要唯一，因为在prepare过程中，所有的acceptor对同一个proposal值只会接收一次，如果两个proposer用相同的proposal值来发起prepare，只有一个会拿到majority。那么，为什么要在开始时尽量用unique的值呢？为了加快收敛的速度。
+ Paxos状态机的回收: 实验允许多个以不同seq标识的Paxos实例，seq线性增长。当需要回收时，我们需要额外的通信。为了避免额外的延迟，我们使用piggyback方式。即，在acceptor accept时，发送自己当前被Done过的seq号码。而Proposer在发送Learn时，将所有成员的Done seq全部发过去，于是，learner在接收时，可以相应地更新，并将自己的min值设置成所有done值的最小值。
+ Propose时没有已知的值: 当一个proposer发起accept时，它可能还不知道应该用什么值（所有prepare\_ok中带的值都是nil），此时，为了简便，直接使用一个垃圾值。
+ 协议的终止: 考虑3个服务器，一个服务器已经达到了一致状态，而在Learn的过程中，发送给另外两个服务器的消息丢失了，此时，它们没有进入一致状态。于是，其他两个会有人超时来发起新的propose，则达到一致的那台服务器会直接返回一个Done消息，表示已经达到一致。于是，proposer会快速收敛。如果proposer没有看到，走常规流程，也依然能够保证它们能够达到相同的一致的值。

# Paxos的缺点
多节点达到一致需要通信，通信开销不可避免。在不考虑任何错误的情况下，PB达成一致只需要`2(n-1)`条消息。而Paxos达成一致需要`2(n-1) + 2(n-1) + (n-1)`条消息。显然，这开销太大了。每次Paxos，它都需要一个Proposer选举的过程，这潜在地增大了所需要的消息数目。在Raft中，由于将选举和日志复制分离，因此，常规日志复制只需要`2(n-1)`条消息。更进一步的，由于使用majority语义，它实际上可以在更少的消息内完成一个Round。
